#### note:
　　该paper为实验性文章，测试了21中激活函数使用MLP、CNN和RNN解决文本方面任务（分类、序列标注）时的效果对比，实验发现，penalized tanh函数在所有任务上表现得最稳定。
 
 #### more:
  1. [代码](https://github.com/UKPLab/emnlp2018-activation-functions)
  2. [21种NLP任务激活函数大比拼：你一定猜不到谁赢了](https://www.jiqizhixin.com/articles/012701)
  3. [26种神经网络激活函数可视化](https://www.jiqizhixin.com/articles/2017-10-10-3)
  4. [一文概览深度学习中的激活函数](https://www.jiqizhixin.com/articles/2017-11-02-26)
  5. [谷歌大脑提出新型激活函数Swish惹争议：可直接替换并优于ReLU？（附机器之心测试）](https://www.jiqizhixin.com/articles/2017-10-21-4)
